{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "662a7edb",
   "metadata": {},
   "source": [
    "# SAS Knowledge Enhancement Using CodeLlama & ChatGPT-4o\n",
    "This notebook demonstrates how to enhance CodeLlama's SAS programming knowledge\n",
    "by leveraging ChatGPT-4o as a teacher model in a knowledge distillation pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b787a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install Required Libraries\n",
    "# Make sure to install the necessary libraries before running this notebook.\n",
    "!pip install transformers datasets openai evaluate peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2bd7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load CodeLlama Model and Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d02df89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pretrained CodeLlama model\n",
    "model_name = \"codellama/CodeLlama-7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb83fdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pipeline for easy text generation\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a56f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Load SAS Validation Dataset\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84369914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your SAS validation dataset\n",
    "validation_data = load_dataset('json', data_files={'validation': 'sas_validation.jsonl'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a1d3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Baseline Evaluation of CodeLlama's SAS Knowledge\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e52f12",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# BLEU score to measure code similarity\n",
    "bleu = evaluate.load(\"bleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3eee24",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the model's performance\n",
    "def evaluate_model(validation_data):\n",
    "    results = []\n",
    "    for item in validation_data['validation']:\n",
    "        prompt = item['prompt']\n",
    "        expected = item['expected_output']\n",
    "\n",
    "        generated = llm_pipeline(prompt, max_length=512)[0]['generated_text']\n",
    "        score = bleu.compute(predictions=[generated], references=[expected])\n",
    "\n",
    "        results.append({\n",
    "            'prompt': prompt,\n",
    "            'expected': expected,\n",
    "            'generated': generated,\n",
    "            'bleu_score': score['bleu']\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507117fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the evaluation\n",
    "eval_results = evaluate_model(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e51e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Identify Knowledge Gaps\n",
    "# Filter outputs where BLEU score is below 0.7\n",
    "gaps = [r for r in eval_results if r['bleu_score'] < 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a522e1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Query ChatGPT-4o for Corrections\n",
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89ffdd9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Set your OpenAI API key\n",
    "openai.api_key = \"YOUR_OPENAI_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fe52af",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Function to query ChatGPT-4o\n",
    "def query_gpt4o(prompt):\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6acd57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Request corrections for identified gaps\n",
    "corrected_data = []\n",
    "for gap in gaps:\n",
    "    correction_prompt = f\"\"\"\n",
    "    Review the following SAS code generated by an AI model. Correct any errors or suggest improvements:\n",
    "    ---\n",
    "    {gap['generated']}\n",
    "    ---\n",
    "    Original question:\n",
    "    {gap['prompt']}\n",
    "    \"\"\"\n",
    "    corrected_output = query_gpt4o(correction_prompt)\n",
    "    corrected_data.append({'prompt': gap['prompt'], 'completion': corrected_output})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfa134e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Fine-Tune CodeLlama with Corrected Data\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e25e2bc9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Convert corrected data into a dataset\n",
    "corrected_dataset = Dataset.from_list(corrected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0408e392",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Tokenization\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['prompt'], truncation=True, padding='max_length', max_length=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b40ba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = corrected_dataset.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafaf28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tuning parameters\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./sas_finetuned_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=4,\n",
    "    num_train_epochs=2,\n",
    "    learning_rate=2e-5,\n",
    "    save_total_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a3f707",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-tune the model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf153ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fine-tuning\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6a8b8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 8: Final Fine-Tuning with Business-Specific SAS Code\n",
    "# Load the business-specific SAS dataset\n",
    "business_data = load_dataset('json', data_files={'train': 'business_sas_data.jsonl'})\n",
    "tokenized_business_data = business_data.map(tokenize, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33f1192a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final fine-tuning\n",
    "final_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_business_data\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95388156",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start the final training phase\n",
    "final_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a271647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 9: Save the Final Model\n",
    "model.save_pretrained(\"./final_business_sas_model\")\n",
    "tokenizer.save_pretrained(\"./final_business_sas_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f5b7f9",
   "metadata": {},
   "source": [
    "This notebook demonstrates an automated loop of evaluating, correcting, and fine-tuning CodeLlama's SAS knowledge,\n",
    "leveraging ChatGPT-4o for high-quality corrections and business-specific knowledge infusion."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
